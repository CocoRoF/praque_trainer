{
    "aclue": [
      "aclue",
      "acp_bench",
      "acp_bool_cot_2shot",
      "acp_mcq_cot_2shot"
    ],
    "aexams": [
      "aexams",
      "aexams_IslamicStudies",
      "aexams_Biology",
      "aexams_Science",
      "aexams_Physics",
      "aexams_Social"
    ],
    "afrimgsm": [
      "afrimgsm",
      "afrimgsm_direct",
      "afrimgsm_en_cot",
      "afrimgsm_translate"
    ],
    "afrimmlu": [
      "afrimmlu",
      "afrimmlu_direct",
      "afrimmlu_translate"
    ],
    "afrixnli": [
      "afrixnli",
      "afrixnli_en_direct",
      "afrixnli_native_direct",
      "afrixnli_translate",
      "afrixnli_manual_direct",
      "afrixnli_manual_translate"
    ],
    "agieval_en": [
      "agieval_en",
      "agieval_aqua_rat",
      "agieval_gaokao_english",
      "agieval_logiqa_en",
      "agieval_lsat_*",
      "agieval_sat_*",
      "agieval_math"
    ],
    "agieval_cn": [
      "agieval_cn",
      "agieval_gaokao_biology",
      "agieval_gaokao_chemistry",
      "agieval_gaokao_chinese",
      "agieval_gaokao_geography",
      "agieval_gaokao_history",
      "agieval_gaokao_mathqa",
      "agieval_gaokao_mathcloze",
      "agieval_gaokao_physics",
      "agieval_jec_qa_ca",
      "agieval_jec_qa_kd",
      "agieval_logiqa_zh"
    ],
    "Alghafa": [
      "copa_ar",
      "piqa_ar"
    ],
    "anli": [
      "anli_r1",
      "anli_r2",
      "anli_r3"
    ],
    "arabic_leaderboard_alghafa": [
      "arabic_leaderboard_alghafa",
      "arabic_leaderboard_arabic_exams",
      "arabic_leaderboard_arabic_mmlu",
      "arabic_leaderboard_arabic_mt_arc_challenge",
      "arabic_leaderboard_arabic_mt_arc_easy",
      "arabic_leaderboard_arabic_mt_boolq",
      "arabic_leaderboard_arabic_mt_copa",
      "arabic_leaderboard_arabic_mt_hellaswag",
      "arabic_leaderboard_arabic_mt_mmlu",
      "arabic_leaderboard_arabic_mt_openbook_qa",
      "arabic_leaderboard_arabic_mt_piqa",
      "arabic_leaderboard_arabic_mt_race",
      "arabic_leaderboard_arabic_mt_sciq",
      "arabic_leaderboard_arabic_mt_toxigen",
      "arabic_leaderboard_acva"
    ],
    "arabic_leaderboard_light": [
      "arabic_leaderboard_light"
    ],
    "arabicmmlu": [
      "arabicmmlu",
      "arabicmmlu_stem",
      "arabicmmlu_stem_social_science",
      "arabicmmlu_stem_humanities",
      "arabicmmlu_stem_language",
      "arabicmmlu_stem_other"
    ],
    "aradice": [
      "aradice"
    ],
    "arc": [
      "arc_easy",
      "arc_challenge"
    ],
    "arc_mt": [
      "arc_challenge_mt_da",
      "arc_challenge_mt_de",
      "arc_challenge_mt_el",
      "arc_challenge_mt_es",
      "arc_challenge_mt_fi",
      "arc_challenge_mt_hu",
      "arc_challenge_mt_is",
      "arc_challenge_mt_it",
      "arc_challenge_mt_nb",
      "arc_challenge_mt_pl",
      "arc_challenge_mt_pt",
      "arc_challenge_mt_sv"
    ],
    "arithmetic": [
      "arithmetic_1dc",
      "arithmetic_2da",
      "arithmetic_2dm",
      "arithmetic_2ds",
      "arithmetic_3da",
      "arithmetic_3ds",
      "arithmetic_4da",
      "arithmetic_4ds",
      "arithmetic_5da",
      "arithmetic_5ds"
    ],
    "asdiv": [
      "asdiv",
      "asdiv_cot_llama"
    ],
    "babi": [
      "babi"
    ],
    "basque_bench": [
      "arc_eu_challenge",
      "arc_eu_easy",
      "belebele_eus_Latn",
      "eus_exams_eu",
      "eus_proficiency",
      "eus_reading",
      "eus_trivia",
      "flores_eu",
      "flores_eu-ca",
      "flores_eu-de",
      "flores_eu-en",
      "flores_eu-es",
      "flores_eu-fr",
      "flores_eu-gl",
      "flores_eu-it",
      "flores_eu-pt",
      "flores_ca-eu",
      "flores_de-eu",
      "flores_en-eu",
      "flores_es-eu",
      "flores_fr-eu",
      "flores_gl-eu",
      "flores_it-eu",
      "flores_pt-eu",
      "mgsm_direct_eu",
      "mgsm_native_cot_eu",
      "paws_eu",
      "piqa_eu",
      "qnlieu",
      "wnli_eu",
      "xcopa_eu",
      "xnli_eu",
      "xnli_eu_native",
      "xstorycloze_eu"
    ],
    "basqueglue": [
      "bhtc_v2",
      "bec2016eu",
      "vaxx_stance",
      "qnlieu",
      "wiceu",
      "epec_koref_bin"
    ],
    "bbh": [
      "bbh_zeroshot",
      "bbh_fewshot",
      "bbh_cot_fewshot",
      "bbh_cot_zeroshot"
    ],
    "bbq": [
      "bbq_age",
      "bbq_disability",
      "bbq_gender",
      "bbq_nationality",
      "bbq_physical_appearance",
      "bbq_race_ethnicity",
      "bbq_religion",
      "bbq_ses",
      "bbq_sexual_orientation"
    ],
    "belebele": [
      "belebele"
    ],
    "bertaqa": [
      "bertaqa_eu",
      "bertaqa_en",
      "bertaqa_en_mt_*"
    ],
    "Bigbench": [
      "bigbench_generate_until",
      "bigbench_multiple_choice_a",
      "bigbench_multiple_choice_b"
    ],
    "careqa": [
      "careqa_en",
      "careqa_es",
      "careqa_open",
      "careqa_open_perplexity"
    ],
    "catalan_bench": [
      "arc_ca_challenge",
      "arc_ca_easy",
      "belebele_cat_Latn",
      "cabreu",
      "catalanqa",
      "catcola",
      "cocoteros_va",
      "copa_ca",
      "coqcat",
      "flores_ca",
      "flores_ca-de",
      "flores_ca-en",
      "flores_ca-es",
      "flores_ca-eu",
      "flores_ca-fr",
      "flores_ca-gl",
      "flores_ca-it",
      "flores_ca-pt",
      "flores_de-ca",
      "flores_en-ca",
      "flores_es-ca",
      "flores_eu-ca",
      "flores_fr-ca",
      "flores_gl-ca",
      "flores_it-ca",
      "flores_pt-ca",
      "mgsm_direct_ca",
      "openbookqa_ca",
      "parafraseja",
      "paws_ca",
      "phrases_ca",
      "piqa_ca",
      "siqa_ca",
      "teca",
      "veritasqa_gen_ca",
      "veritasqa_mc1_ca",
      "veritasqa_mc2_ca",
      "wnli_ca",
      "xnli_ca",
      "xquad_ca",
      "xstorycloze_ca"
    ],
    "ceval": [
      "ceval-valid_accountant",
      "ceval-valid_advanced_mathematics",
      "ceval-valid_art_studies",
      "ceval-valid_basic_medicine",
      "ceval-valid_business_administration",
      "ceval-valid_chinese_language_and_literature",
      "ceval-valid_civil_servant",
      "ceval-valid_clinical_medicine",
      "ceval-valid_college_chemistry",
      "ceval-valid_college_economics",
      "ceval-valid_college_physics",
      "ceval-valid_college_programming",
      "ceval-valid_computer_architecture",
      "ceval-valid_computer_network",
      "ceval-valid_discrete_mathematics",
      "ceval-valid_education_science",
      "ceval-valid_electrical_engineer",
      "ceval-valid_environmental_impact_assessment_engineer",
      "ceval-valid_fire_engineer",
      "ceval-valid_high_school_biology",
      "ceval-valid_high_school_chemistry",
      "ceval-valid_high_school_chinese",
      "ceval-valid_high_school_geography",
      "ceval-valid_high_school_mathematics",
      "ceval-valid_high_school_physics",
      "ceval-valid_high_school_politics",
      "ceval-valid_ideological_and_moral_cultivation",
      "ceval-valid_law",
      "ceval-valid_legal_professional",
      "ceval-valid_logic",
      "ceval-valid_mao_zedong_thought",
      "ceval-valid_marxism",
      "ceval-valid_metrology_engineer",
      "ceval-valid_middle_school_biology",
      "ceval-valid_middle_school_chemistry",
      "ceval-valid_middle_school_geography",
      "ceval-valid_middle_school_history",
      "ceval-valid_middle_school_mathematics",
      "ceval-valid_middle_school_physics",
      "ceval-valid_middle_school_politics",
      "ceval-valid_modern_chinese_history",
      "ceval-valid_operating_system",
      "ceval-valid_physician",
      "ceval-valid_plant_protection",
      "ceval-valid_probability_and_statistics",
      "ceval-valid_professional_tour_guide",
      "ceval-valid_sports_science",
      "ceval-valid_tax_accountant",
      "ceval-valid_teacher_qualification",
      "ceval-valid_urban_and_rural_planner",
      "ceval-valid_veterinary_medicine"
    ],
    "chartqa": [
      "chartqa_llama",
      "chartqa_llama_90"
    ],
    "cmmlu": [
      "cmmlu_agronomy",
      "cmmlu_anatomy",
      "cmmlu_ancient_chinese",
      "cmmlu_arts",
      "cmmlu_astronomy",
      "cmmlu_business_ethics",
      "cmmlu_chinese_civil_service_exam",
      "cmmlu_chinese_driving_rule",
      "cmmlu_chinese_food_culture",
      "cmmlu_chinese_foreign_policy",
      "cmmlu_chinese_history",
      "cmmlu_chinese_literature",
      "cmmlu_chinese_teacher_qualification",
      "cmmlu_clinical_knowledge",
      "cmmlu_college_actuarial_science",
      "cmmlu_college_education",
      "cmmlu_college_engineering_hydrology",
      "cmmlu_college_law",
      "cmmlu_college_mathematics",
      "cmmlu_college_medical_statistics",
      "cmmlu_college_medicine",
      "cmmlu_computer_science",
      "cmmlu_computer_security",
      "cmmlu_conceptual_physics",
      "cmmlu_construction_project_management",
      "cmmlu_default_agronomy",
      "cmmlu_default_anatomy",
      "cmmlu_default_ancient_chinese",
      "cmmlu_default_arts",
      "cmmlu_default_astronomy",
      "cmmlu_default_business_ethics",
      "cmmlu_default_chinese_civil_service_exam",
      "cmmlu_default_chinese_driving_rule",
      "cmmlu_default_chinese_food_culture",
      "cmmlu_default_chinese_foreign_policy",
      "cmmlu_default_chinese_history",
      "cmmlu_default_chinese_literature",
      "cmmlu_default_chinese_teacher_qualification",
      "cmmlu_default_clinical_knowledge",
      "cmmlu_default_college_actuarial_science",
      "cmmlu_default_college_education",
      "cmmlu_default_college_engineering_hydrology",
      "cmmlu_default_college_law",
      "cmmlu_default_college_mathematics",
      "cmmlu_default_college_medical_statistics",
      "cmmlu_default_college_medicine",
      "cmmlu_default_computer_science",
      "cmmlu_default_computer_security",
      "cmmlu_default_conceptual_physics",
      "cmmlu_default_construction_project_management",
      "cmmlu_default_economics",
      "cmmlu_default_education",
      "cmmlu_default_electrical_engineering",
      "cmmlu_default_elementary_chinese",
      "cmmlu_default_elementary_commonsense",
      "cmmlu_default_elementary_information_and_technology",
      "cmmlu_default_elementary_mathematics",
      "cmmlu_default_ethnology",
      "cmmlu_default_food_science",
      "cmmlu_default_genetics",
      "cmmlu_default_global_facts",
      "cmmlu_default_high_school_biology",
      "cmmlu_default_high_school_chemistry",
      "cmmlu_default_high_school_geography",
      "cmmlu_default_high_school_mathematics",
      "cmmlu_default_high_school_physics",
      "cmmlu_default_high_school_politics",
      "cmmlu_default_human_sexuality",
      "cmmlu_default_international_law",
      "cmmlu_default_journalism",
      "cmmlu_default_jurisprudence",
      "cmmlu_default_legal_and_moral_basis",
      "cmmlu_default_logical",
      "cmmlu_default_machine_learning",
      "cmmlu_default_management",
      "cmmlu_default_marketing",
      "cmmlu_default_marxist_theory",
      "cmmlu_default_modern_chinese",
      "cmmlu_default_nutrition",
      "cmmlu_default_philosophy",
      "cmmlu_default_professional_accounting",
      "cmmlu_default_professional_law",
      "cmmlu_default_professional_medicine",
      "cmmlu_default_professional_psychology",
      "cmmlu_default_public_relations",
      "cmmlu_default_security_study",
      "cmmlu_default_sociology",
      "cmmlu_default_sports_science",
      "cmmlu_default_traditional_chinese_medicine",
      "cmmlu_default_virology",
      "cmmlu_default_world_history",
      "cmmlu_default_world_religions",
      "cmmlu_economics",
      "cmmlu_education",
      "cmmlu_electrical_engineering",
      "cmmlu_elementary_chinese",
      "cmmlu_elementary_commonsense",
      "cmmlu_elementary_information_and_technology",
      "cmmlu_elementary_mathematics",
      "cmmlu_ethnology",
      "cmmlu_food_science",
      "cmmlu_genetics",
      "cmmlu_global_facts",
      "cmmlu_high_school_biology",
      "cmmlu_high_school_chemistry",
      "cmmlu_high_school_geography",
      "cmmlu_high_school_mathematics",
      "cmmlu_high_school_physics",
      "cmmlu_high_school_politics",
      "cmmlu_human_sexuality",
      "cmmlu_international_law",
      "cmmlu_journalism",
      "cmmlu_jurisprudence",
      "cmmlu_legal_and_moral_basis",
      "cmmlu_logical",
      "cmmlu_machine_learning",
      "cmmlu_management",
      "cmmlu_marketing",
      "cmmlu_marxist_theory",
      "cmmlu_modern_chinese",
      "cmmlu_nutrition",
      "cmmlu_philosophy",
      "cmmlu_professional_accounting",
      "cmmlu_professional_law",
      "cmmlu_professional_medicine",
      "cmmlu_professional_psychology",
      "cmmlu_public_relations",
      "cmmlu_security_study",
      "cmmlu_sociology",
      "cmmlu_sports_science",
      "cmmlu_traditional_chinese_medicine",
      "cmmlu_virology",
      "cmmlu_world_history",
      "cmmlu_world_religions"
    ],
    "commonsense_qa": [
      "commonsense_qa"
    ],
    "copal_id": [
      "copal_id_standard",
      "copal_id_colloquial"
    ],
    "coqa": [
      "coqa"
    ],
    "crows_pairs_english": [
      "crows_pairs_english_age",
      "crows_pairs_english_autre",
      "crows_pairs_english_disabilitycrows_pairs_english_gender",
      "crows_pairs_english_nationality",
      "crows_pairs_english_physical_appearance",
      "crows_pairs_english_race_color",
      "crows_pairs_english_religion",
      "crows_pairs_english_sexual_orientation",
      "crows_pairs_english_socioeconomic"
    ],
    "crows_pairs_french": [
      "crows_pairs_french_age",
      "crows_pairs_french_autre",
      "crows_pairs_french_disability",
      "crows_pairs_french_gender",
      "crows_pairs_french_nationality",
      "crows_pairs_french_physical_appearance",
      "crows_pairs_french_race_color",
      "crows_pairs_french_religion",
      "crows_pairs_french_sexual_orientation",
      "crows_pairs_french_socioeconomic"
    ],
    "Csatqa": [
      "csatqa_gr",
      "csatqa_li",
      "csatqa_rch",
      "csatqa_rcs",
      "csatqa_rcss",
      "csatqa_wr"
    ],
    "darija_sentiment": [
      "darija_sentiment_mac",
      "darija_sentiment_myc",
      "darija_sentiment_msac",
      "darija_sentiment_msda",
      "darija_sentiment_electrom"
    ],
    "darija_summarization": [
      "darija_summarization_task"
    ],
    "darija_translation": [
      "darija_translation_doda",
      "darija_translation_flores",
      "darija_translation_madar",
      "darija_translation_seed"
    ],
    "darija_transliteration": [
      "darija_transliteration_task"
    ],
    "darijahellaswag": [
      "darijahellaswag"
    ],
    "drop": [
      "drop"
    ],
    "eq_bench": [
      "eq_bench"
    ],
    "eus_exams": [
      "eus_exams_eu",
      "eus_exams_es"
    ],
    "eus_proficiency": [
      "eus_proficiency"
    ],
    "eus_reading": [
      "eus_reading"
    ],
    "eus_trivia": [
      "eus_trivia"
    ],
    "evalita_llm": [
      "evalita-mp_te",
      "evalita-mp_sa",
      "evalita-mp_wic",
      "evalita-mp_hs",
      "evalita-mp_at",
      "evalita-mp_faq",
      "evalita-mp_sum_fp",
      "evalita-mp_ls",
      "evalita-mp_ner_group",
      "evalita-mp_re"
    ],
    "fda": [
      "fda"
    ],
    "fld": [
      "fld_default",
      "fld_star"
    ],
    "fld_logical_formula": [
      "fld_logical_formula_default",
      "fld_logical_formula_fld_star"
    ],
    "french_bench": [
      "french_bench_boolqa",
      "french_bench_fquadv2",
      "french_bench_fquadv2_bool",
      "french_bench_fquadv2_genq",
      "french_bench_fquadv2_hasAns",
      "french_bench_topic_based_nli",
      "french_bench_multifquad",
      "french_bench_grammar",
      "french_bench_vocab",
      "french_bench_reading_comp",
      "french_bench_xnli",
      "french_bench_orangesum_abstract",
      "french_bench_orangesum_title",
      "french_bench_trivia",
      "french_bench_hellaswag",
      "french_bench_arc_challenge"
    ],
    "flores_gl": [
      "belebele_glg_Latn",
      "flores_gl",
      "flores_gl-ca",
      "flores_gl-de",
      "flores_gl-en",
      "flores_gl-es",
      "flores_gl-eu",
      "flores_gl-fr",
      "flores_gl-it",
      "flores_gl-pt",
      "flores_ca-gl",
      "flores_de-gl",
      "flores_en-gl",
      "flores_es-gl",
      "flores_eu-gl",
      "flores_fr-gl",
      "flores_it-gl",
      "flores_pt-gl",
      "galcola",
      "summarization_gl",
      "parafrases_gl",
      "paws_gl",
      "openbookqa_gl",
      "mgsm_direct_gl",
      "truthfulqa_gl",
      "xnli_gl",
      "xstorycloze_gl"
    ],
    "glianorex": [
      "glianorex_en",
      "glianorex_fr"
    ],
    "global_mmlu": [
      "global_mmlu_ar",
      "global_mmlu_bn",
      "global_mmlu_de",
      "global_mmlu_en",
      "global_mmlu_es",
      "global_mmlu_fr",
      "global_mmlu_hi",
      "global_mmlu_id",
      "global_mmlu_it",
      "global_mmlu_ja",
      "global_mmlu_ko",
      "global_mmlu_pt",
      "global_mmlu_sw",
      "global_mmlu_yo",
      "global_mmlu_zh"
    ],
    "global_mmlu_full": [
      "global_mmlu_full_am",
      "global_mmlu_full_ar",
      "global_mmlu_full_bn",
      "global_mmlu_full_cs",
      "global_mmlu_full_de",
      "global_mmlu_full_el",
      "global_mmlu_full_en",
      "global_mmlu_full_es",
      "global_mmlu_full_fa",
      "global_mmlu_full_fil",
      "global_mmlu_full_fr",
      "global_mmlu_full_ha",
      "global_mmlu_full_he",
      "global_mmlu_full_hi",
      "global_mmlu_full_id",
      "global_mmlu_full_ig",
      "global_mmlu_full_it",
      "global_mmlu_full_ja",
      "global_mmlu_full_ko",
      "global_mmlu_full_ky",
      "global_mmlu_full_lt",
      "global_mmlu_full_mg",
      "global_mmlu_full_ms",
      "global_mmlu_full_ne",
      "global_mmlu_full_nl",
      "global_mmlu_full_ny",
      "global_mmlu_full_pl",
      "global_mmlu_full_pt",
      "global_mmlu_full_ro",
      "global_mmlu_full_ru",
      "global_mmlu_full_si",
      "global_mmlu_full_sn",
      "global_mmlu_full_so",
      "global_mmlu_full_sr",
      "global_mmlu_full_sv",
      "global_mmlu_full_sw",
      "global_mmlu_full_te",
      "global_mmlu_full_tr",
      "global_mmlu_full_uk",
      "global_mmlu_full_vi",
      "global_mmlu_full_yo",
      "global_mmlu_full_zh"
    ],
    "glue": [
      "cola",
      "mnli",
      "mrpc",
      "qnli",
      "qqp",
      "rte",
      "sst",
      "wnli"
    ],
    "piqa": [
      "piqa"
    ],
    "nq_open": [
      "nq_open"
    ],
    "mmlu_pro": [
      "mmlu_pro_biology",
      "mmlu_pro_business",
      "mmlu_pro_chemistry",
      "mmlu_pro_computer_science",
      "mmlu_pro_economics",
      "mmlu_pro_engineering",
      "mmlu_pro_health",
      "mmlu_pro_history",
      "mmlu_pro_law",
      "mmlu_pro_math",
      "mmlu_pro_other",
      "mmlu_pro_philosophy",
      "mmlu_pro_physics",
      "mmlu_pro_psychology"
    ],
    "mmlu": [
      "mmlu",
      "mmlu_continuation",
      "mmlu_generation"
    ],
    "mmlu_pro_plus": [
      "mmlu_pro_plus",
      "mmlu_pro_plus_biology",
      "mmlu_pro_plus_business",
      "mmlu_pro_plus_chemistry",
      "mmlu_pro_plus_computer_science",
      "mmlu_pro_plus_economics",
      "mmlu_pro_plus_engineering",
      "mmlu_pro_plus_health",
      "mmlu_pro_plus_history",
      "mmlu_pro_plus_law",
      "mmlu_pro_plus_math",
      "mmlu_pro_plus_other",
      "mmlu_pro_plus_philosophy",
      "mmlu_pro_plus_physics",
      "mmlu_pro_plus_psychology"
    ],
    "gsm8k": [
      "gsm8k_yaml",
      "gsm8k_cot",
      "gsm8k_cot_self_consistency",
      "gsm8k_cot_llama"
    ],
    "gpqa": [
      "gpqa_main_zeroshot",
      "gpqa_main_n_shot",
      "gpqa_main_generative_n_shot",
      "gpqa_main_cot_zeroshot",
      "gpqa_main_cot_n_shot"
    ],
    "gpqa_diamond": [
      "gpqa_diamond_zeroshot",
      "gpqa_diamond_n_shot",
      "gpqa_diamond_generative_n_shot",
      "gpqa_diamond_cot_zeroshot",
      "gpqa_diamond_cot_n_shot"
    ],
    "gpqa_extended": [
      "gpqa_extended_zeroshot",
      "gpqa_extended_n_shot",
      "gpqa_extended_generative_n_shot",
      "gpqa_extended_cot_zeroshot",
      "gpqa_extended_cot_n_shot"
    ],
    "humaneval": [
      "humaneval",
      "humaneval_64",
      "humaneval_instruct",
      "humaneval_instruct_64"
    ],
    "mbpp": [
      "mbpp"
    ],
    "mgsm_direct": [
      "mgsm_direct",
      "mgsm_direct_bn",
      "mgsm_direct_de",
      "mgsm_direct_en",
      "mgsm_direct_es",
      "mgsm_direct_fr",
      "mgsm_direct_ja",
      "mgsm_direct_ru",
      "mgsm_direct_sw",
      "mgsm_direct_te",
      "mgsm_direct_th",
      "mgsm_direct_zh"
    ],
    "mgsm_cot_native": [
      "mgsm_cot_native",
      "mgsm_cot_native_bn",
      "mgsm_cot_native_de",
      "mgsm_cot_native_en",
      "mgsm_cot_native_es",
      "mgsm_cot_native_fr",
      "mgsm_cot_native_ja",
      "mgsm_cot_native_ru",
      "mgsm_cot_native_sw",
      "mgsm_cot_native_te",
      "mgsm_cot_native_th",
      "mgsm_cot_native_zh"
    ],
    "Wet": [
      "gpt3_translation_tasks",
      "wmt14",
      "wmt16",
      "wmt20",
      "iwslt2017"
    ],
    "xquad": [
   "xquad_ar",
   "xquad_de",
   "xquad_el",
   "xquad_en",
   "xquad_es",
   "xquad_hi",
   "xquad_ro",
   "xquad_ru",
   "xquad_th",
   "xquad_tr",
   "xquad_vi",
   "xquad_zh"
 ]
}