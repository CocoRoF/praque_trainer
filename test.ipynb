{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf358348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download from the 🤗 Hub\n",
    "model = SentenceTransformer(\"CocoRoF/POLAR-Qwen3-0.6b-linq-gist\", tokenizer_kwargs={\"padding_side\": \"left\"},)\n",
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda913d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\", tokenizer_kwargs={\"padding_side\": \"left\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07490515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8819, 0.7349]])\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    '세계는 남자가 중심이고 이 세상을 지배하는 자들이다'\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    \"세계는 여자가 중심이고 이 세상을 지배하는 자들이다\",\n",
    "    \"나는 아빠가 좋다. 역시 가부장적 사고가 필요해 엄마 미워\"\n",
    "]\n",
    "\n",
    "query_embeddings = model.encode(queries, prompt_name=\"query\")\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "similarity = model.similarity(query_embeddings, document_embeddings)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81a21727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6607, 0.7754, 0.5711, 0.5999]])\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    '선불카드를 미사용하는 경우 잔액이 사라질 수도 있나?'\n",
    "]\n",
    "\n",
    "documents = [\n",
    "\"\"\"제2조 (정의) 이 약관에서 사용하는 용어의 정의는 다음과 같습니다. \"회원\"이란 이 약관을 승인하고 카드사에 기명식 선불카드 발급을 신청하여, 카드사로부터 기명식 선불카드를 발급받아 사용하는 자를 말합니다. \"소지자\"란 이 약관을 승인하고 카드사에 무기명식 선불카드의 구매를 신청한 후 카드사로부터 무기명식 선불카드를 구매하여 소지하거나, 구매한 무기명식 선불카드를 사용하는 자를 말합니다. \"선불카드\"란 카드사가 대금을 미리 받고 이에 해당하는 금액을 기록(전자적 또는 자기적 방법에 따른 기록을 말한다)하여 발행한 증표 또는 그 증표에 관한 정보로서 회원 등이 가맹점에 제시하여 그 카드에 기록된 금액의 범위에서 결제할 수 있게 한 증표 또는 그 증표에 관한 정보를 말합니다. \"기명식 선불카드\"란 회원이 카드사에 신청하여 발급받은 선불카드로서 카드실물에 회원의 성명이 인쇄되어 있거나 카드사 전산에 기명식 회원으로서의 정보가 존재하는 카드를 의미하고 발급 이후 양도가 불가능합니다. \"무기명식 선불카드\"란 고객이 카드사에 신청하여 구매한 선불카드로서 카드 실물에 성명이 미인쇄되어 있으며 카드사 전산에 기명식 회원으로서의 정보가 존재하지 않고 양도가 가능한 선불카드를 말합니다. 단, 무기명식 선불카드의 소지자가 인터넷 사용등록, 소득공제 등록, 카드이용내역 안내 휴대폰 문자메시지 서비스를 신청한 경우 카드사 전산에 해당 소지자의 정보가 존재할 수 있습니다. \"충전\"이란 선불카드로 구매행위를 하기 위해 권면금액 또는 관계법령 및 이 약관이 정한 범위 내에서 회원 등이 원하는 만큼의 금액을 카드사 영업점, 홈페이지 등을 통해 카드사가 정한 방법에 의하여 사용 가능한 상태로 바꾸는 것을 말합니다. \"선불카드의 유효기한\"이란 카드사로부터 선불카드를 발급받아 가맹점에 제시하여 사용할 수 있는 기간을 말합니다. \"소멸시효기간\"이란 회원 등이 카드사에 대하여 선불카드상 충전된 금액의 사용 및 반환을 요구할 수 있는 기간을 말합니다. 제 2 장 선불카드의 발급 및 이용 등 제3조 (선불카드의 발급 및 구매) 회원 등이 기명식 선불카드의 발급 또는 무기명식 선불카드의 구매를 희망할 경우 지정판매처 등을 통해 카드사가 정한 방법으로 신청할 수 있으며, 선불카드의 발급매수 및 발급자격은 카드사가 별도로 정할 수 있습니다. 회원 등이 선불카드의 발급 및 구매를 신청할 경우 카드사는 별도의 제작비용을, 배송이 필요한 경우 별도의 배송료를 청구할 수 있습니다. 선불카드에 대한 별도의 연회비는 없으며, 선불카드를 발급(재발급)받는 경우 카드사는 선불카드의 발급에 우선하여 발급수수료 등을 받을 수 있습니다. 카드사는 선불카드를 발급하거나 판매하는 경우 회원 등에게 부가서비스, 이용한도, 부대비용, 사용불가·제한 가맹점명 및 거래유형에 관한 목록, 인터넷 사용등록·소득공제의 방법 등 중요사항에 대해 설명합니다. 제4조(선불카드의 유효기한 및 재발급) 카드사는 선불카드 발급 시 전산 관리 및 인터넷 거래 등을 위하여 선불카드 표면에 유효기한을 기재하여 교부합니다.\"\"\",\n",
    "\"\"\"제5조(선불카드와 소멸시효) 기사용 선불카드의 잔액은 최종사용월로부터, 미사용 선불카드의 잔액은 판매월(또는 충전월)로부터 5년이 경과하면 소멸됩니다. 다만, 소멸시효기간은 5년보다 길게 정할 수 있습니다. 카드사는 소멸시효가 완성된 선불카드의 미사용 잔액을 여신전문금융업협회가 설립한 기부금관리재단에 기부할 수 있습니다. 다만, 카드사는 5만원 이상의 기명식 선불카드 미사용 잔액을 기부하는 경우 기부하기 1개월 전까지 다음 각 호의 사항을 포함하여 서면(書面), 우편, 전자우편(E-MAIL), 전화, 팩스, 휴대전화 문자메시지, 전자문서 중 한 가지 이상의 방법으로 회원에게 통지합니다. 기부금 액수 기부예정일 기부처 기부에 관한 통지에 대하여 회원은 통지받은 날로부터 30일 이내에 이의를 제기할 수 있으며, 해당기간 내 별도의 이의를 제기하지 아니하는 경우 기부에 동의한 것으로 본다는 사실 제2항에 따라 기부예정 통지를 한 카드사는 서명(전자서명 포함), 기명날인, 녹취, 전화자동응답시스템, 휴대폰 문자메시지 서비스 중 하나의 방법으로 회원에게 동의를 얻어야 합니다. 다만, 통지를 받은 회원이 30일 이내에 이의를 제기하지 않는 경우에는 기부에 동의한 것으로 봅니다. 제6조(선불카드의 이용) 선불카드는 일시불 구매용으로만 사용이 가능하며, 회원 등이 선불카드로 상품을 구매하거나 서비스를 제공받고자 할 때에는 국내의 경우에는 카드사 또는 카드사와 제휴한 기관의 가맹점(이하 \"국내가맹점\"이라 함), 국외의 경우에는 카드사와 제휴하고 있는 외국기관의 가맹점(이하 \"해외가맹점\"이라 함)에서 사용할 수 있습니다. 선불카드 회원 등은 충전된 선불카드 잔액 범위 내에서 사용할 수 있습니다. 회원 등이 선불카드 사용 시 카드사는 해당 선불카드의 충전금액에서 결제금액만큼을 즉시 차감합니다. 기명식 선불카드 회원은 선불카드를 제시하고 매출전표에 선불카드 상의 서명과 동일한 서명을, 무기명식 선불카드 소지자는 선불카드를 제시하고 매출전표에 본인의 서명을 하여야 합니다. 전자상거래, 통신판매 등에 있어서 가맹점이 본인확인을 할 수 있는 다른 방법이 있는 경우이거나 선불카드의 제시와 서명 생략으로 입을 수 있는 소지자의 피해를 카드사 또는 가맹점이 부담하는 경우에는 제4항에 따른 서명을 생략할 수 있습니다. 제7조(선불카드의 이용 제한) 선불카드는 카드사의 가맹점에서 신용카드와 동일하게 사용할 수 있으나, 가맹점의 결제거부·제한 등으로 일부 사용이 제한될 수 있으며, 카드사는 가맹점이 선불카드 결제를 거부하거나 제한하는 것을 알게 된 경우, 지체 없이 회사 홈페이지 등을 통해 해당 가맹점 및 대상 거래 유형을 알려드립니다. 카드사는 다음 각 호의 어느 하나에 해당하는 경우에는 회원 등의 선불카드 사용을 제한할 수 있으며, 카드사 홈페이지 등을 통해 사용이 제한된 가맹점을 고지합니다. 선불카드 특성상 취급에 어려움이 있는 무승인 거래(항공기 내 구매 등)의 경우 선불카드 상품 출시 시, 특정 기관과 제휴를 통해 사용처를 제한하는 경우\"\"\",\n",
    "\"\"\"제17조(이용약관의 변경) 이 약관을 변경할 경우 카드사는 그 내용을 변경 약관 시행일로부터 1개월 이전까지 카드사 홈페이지에 게시(기존 가입자에 대한 변경약관의 적용여부, 신.구대비표 포함)하고 회원 등(단, 무기명식 선불카드 소지자의 경우 인터넷 사용등록, 소득공제 등록, 카드이용내역 안내 휴대폰 문자메시지 서비스 신청으로 카드사에 해당 소지자의 정보가 존재하는 경우에 한함)에게 서면(書面), 우편, 전자우편(E-MAIL), 전화, 팩스, 휴대전화 문자메시지, 전자문서 중 한 가지 이상의 방법으로 개별통지(신·구대비표 포함)합니다. 다만, 다음 각 호의 어느 하나에 해당하는 경우에는 변경된 약관을 즉시 게시 및 개별 통지하여 드립니다. 법령 개정, 제도 개선, 약관 변경권고(명령) 등으로 긴급히 약관을 변경한 경우 약관 개정이 회원에게 유리한 경우 변경 전 내용이 기존 회원에게 그대로 적용되는 경우 기존 약관의 내용이 실질적으로 변경되지 않는 단순한 문구 변경 전항의 경우 회원 등이 변경에 동의하지 않는 경우 통지일로부터 1개월 이내에 계약을 해지할 수 있으며, 계약해지의 의사표시를 않는 경우에는 변경에 동의한 것으로 본다는 내용을 명시하여 게시 또는 통지합니다. 회원 등이 변경예정일까지 이의를 제기하지 않았을 때에는 변경된 약관을 승인한 것으로 간주합니다. 제18조(약관에서 정하지 아니한 사항) 신용정보의 제공·이용, 약관 위반 시의 책임, 관할법원 등 이 약관에서 정하지 아니한 사항 및 그 해석에 관하여는 신용카드 개인회원 표준약관, 관계법령 또는 상관례를 따릅니다. 제 3 장 기명식 선불카드 제19조(기명식 선불카드의 관리) 회원은 발급받은 기명식 선불카드를 수령한 즉시 카드 서명란에 본인이 직접 서명하여야 하며 회원 본인 이외의 제3자로 하여금 선불카드를 보관 또는 소지하게 하거나 이용하게 하여서는 안 됩니다. 회원은 기명식 선불카드를 제3자에게 대여하거나 양도 또는 담보의 목적으로 이용할 수 없으며, 선량한 관리자로서의 주의를 다하여 선불카드를 이용·관리하여야 합니다. 제 4 장 무기명식 선불카드 제20조(무기명식 선불카드의 관리) 무기명식 선불카드의 발행권면금액 또는 충전된 금액을 모두 이용한 이후 매출취소를 할 경우에는 카드실물이 필요하므로 소지자는 카드실물을 일정기간 보관한 후 폐기하여야 합니다. 다만, 카드실물 없이 영수증 등을 통해 매출취소를 요청하는 경우, 취소 대상 매출 발생 시 정당한 선불카드 소지자임이 확인되면 매출취소가 가능합니다. 제21조(무기명식 선불카드의 인터넷 사용등록) 소지자는 선불카드를 카드사의 가맹점에서 별도의 비밀번호 없이 사용할 수 있으나, 인터넷을 이용한 전자상거래시에는 사전에 카드사 홈페이지 등을 통해 인터넷 사용등록 후 이용하여야 합니다.\"\"\",\n",
    "\"\"\"회원 등이 제3호에 따른 추가적인 보안조치에 사용되는 매체·수단 또는 정보에 대하여 다음 각 목의 어느 하나에 해당하는 행위를 하여 전자금융거래를 위한 전자적 장치 또는 정보통신망에 침입한 제3자가 거짓이나 그 밖의 부정한 방법으로 획득한 접근매체를 이용하여 발생한 사고의 경우 가. 누설·노출 또는 방치한 행위 나. 제3자에게 대여하거나 그 사용을 위임한 행위 또는 양도나 담보의 목적으로 제공한 행위 회원 등은 제2항 각 호와 관련하여 사고조사가 필요한 경우 카드사의 요구에 협조하여야 합니다. 제15조(변경사항의 통지) 기명식 선불카드 회원은 주소, 전화번호, 전자우편(E-MAIL) 등의 변경사항이 있을 때 카드사에 즉시 통지하여야 합니다. 무기명식 선불카드 소지자 중 인터넷 사용등록, 소득공제등록, 카드이용내역 안내 휴대폰 문자메시지 서비스를 신청하여 소지자의 정보를 등록한 경우, 등록정보의 변경사항이 있을 때 카드사에 즉시 통지하여야 합니다. 회원 등이 제1항 및 제2항의 통지를 태만히 하여 카드사가 과실 없이 회원 등의 변경된 주소 등을 알지 못하는 경우에 한하여, 카드사로부터 통지 또는 송부서류 등이 늦게 도착하거나 도착하지 않음으로 인하여 발생한 손해는 회원 등이 부담하여야 합니다. 이 경우 통상 도착하여야 할 때에 회원 등에게 도착한 것으로 하여 그 도착으로 인한 법률효과가 발생합니다. 제16조(이용약관의 효력) 이 약관의 내용은 카드사가 제공하는 인터넷 홈페이지 등에 게시하거나 선불카드의 구매·발급 시 약관을 교부하는 등의 방법으로 회원 등에게 공지함으로써 효력을 발생합니다. 단, 무기명식 선불카드 양도의 경우 양도한 때에 이 약관의 효력이 발생 되는 것으로 간주합니다.\"\"\"\n",
    "]\n",
    "\n",
    "query_embeddings = model.encode(queries, prompt_name=\"query\")\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "similarity = model.similarity(query_embeddings, document_embeddings)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af7eed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f577b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model_a = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc3100a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[-14.2266,  -1.1276,   4.6604,  ..., -14.7097, -14.7547, -14.6708],\n",
       "         [-11.2772,  -0.6546,  -5.0065,  ..., -11.6818, -11.6677, -11.6955],\n",
       "         [-22.6223,   6.3905,   4.9708,  ..., -23.9000, -23.9398, -24.0407],\n",
       "         ...,\n",
       "         [ -8.5231,   6.3015,  -3.5652,  ..., -10.6850, -10.6294, -10.6779],\n",
       "         [-22.1560,   3.1504,  12.3382,  ..., -22.5151, -22.5003, -22.3365],\n",
       "         [-15.3647, -13.0501,  -1.1583,  ..., -16.5285, -16.7630, -16.4063]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.HybridCache object at 0x000002848E8B8B10>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53e6ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen: vision_tower.vision_model.embeddings.patch_embedding.weight\n",
      "Frozen: vision_tower.vision_model.embeddings.patch_embedding.bias\n",
      "Frozen: vision_tower.vision_model.embeddings.position_embedding.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.0.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.1.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.2.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.3.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.4.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.5.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.6.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.7.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.8.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.9.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.10.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.11.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.12.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.13.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.14.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.15.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.16.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.17.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.18.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.19.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.20.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.21.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.22.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.23.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.24.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.25.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.bias\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight\n",
      "Unfrozen: vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.layer_norm1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.layer_norm1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.mlp.fc1.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.mlp.fc2.bias\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.layer_norm2.weight\n",
      "Frozen: vision_tower.vision_model.encoder.layers.26.layer_norm2.bias\n",
      "Frozen: vision_tower.vision_model.post_layernorm.weight\n",
      "Frozen: vision_tower.vision_model.post_layernorm.bias\n",
      "Frozen: multi_modal_projector.mm_input_projection_weight\n",
      "Frozen: multi_modal_projector.mm_soft_emb_norm.weight\n",
      "Unfrozen: language_model.model.embed_tokens.weight\n",
      "Frozen: language_model.model.layers.0.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.0.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.0.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.0.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.0.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.0.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.0.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.0.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.0.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.0.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.0.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.0.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.0.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.1.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.1.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.1.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.1.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.1.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.1.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.1.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.1.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.1.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.1.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.1.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.1.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.1.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.2.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.2.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.2.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.2.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.2.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.2.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.2.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.2.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.2.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.2.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.2.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.2.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.2.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.3.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.3.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.3.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.3.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.3.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.3.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.3.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.3.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.3.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.3.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.3.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.3.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.3.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.4.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.4.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.4.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.4.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.4.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.4.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.4.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.4.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.4.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.4.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.4.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.4.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.4.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.5.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.5.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.5.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.5.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.5.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.5.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.5.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.5.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.5.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.5.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.5.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.5.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.5.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.6.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.6.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.6.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.6.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.6.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.6.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.6.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.6.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.6.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.6.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.6.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.6.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.6.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.7.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.7.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.7.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.7.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.7.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.7.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.7.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.7.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.7.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.7.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.7.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.7.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.7.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.8.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.8.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.8.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.8.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.8.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.8.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.8.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.8.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.8.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.8.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.8.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.8.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.8.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.9.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.9.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.9.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.9.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.9.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.9.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.9.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.9.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.9.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.9.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.9.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.9.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.9.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.10.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.10.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.10.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.10.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.10.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.10.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.10.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.10.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.10.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.10.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.10.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.10.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.10.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.11.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.11.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.11.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.11.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.11.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.11.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.11.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.11.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.11.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.11.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.11.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.11.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.11.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.12.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.12.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.12.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.12.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.12.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.12.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.12.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.12.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.12.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.12.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.12.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.12.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.12.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.13.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.13.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.13.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.13.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.13.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.13.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.13.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.13.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.13.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.13.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.13.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.13.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.13.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.14.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.14.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.14.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.14.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.14.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.14.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.14.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.14.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.14.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.14.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.14.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.14.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.14.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.15.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.15.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.15.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.15.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.15.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.15.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.15.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.15.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.15.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.15.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.15.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.15.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.15.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.16.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.16.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.16.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.16.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.16.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.16.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.16.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.16.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.16.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.16.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.16.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.16.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.16.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.17.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.17.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.17.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.17.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.17.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.17.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.17.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.17.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.17.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.17.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.17.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.17.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.17.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.18.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.18.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.18.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.18.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.18.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.18.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.18.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.18.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.18.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.18.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.18.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.18.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.18.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.19.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.19.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.19.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.19.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.19.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.19.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.19.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.19.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.19.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.19.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.19.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.19.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.19.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.20.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.20.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.20.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.20.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.20.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.20.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.20.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.20.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.20.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.20.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.20.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.20.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.20.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.21.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.21.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.21.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.21.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.21.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.21.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.21.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.21.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.21.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.21.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.21.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.21.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.21.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.22.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.22.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.22.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.22.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.22.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.22.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.22.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.22.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.22.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.22.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.22.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.22.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.22.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.23.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.23.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.23.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.23.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.23.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.23.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.23.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.23.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.23.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.23.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.23.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.23.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.23.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.24.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.24.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.24.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.24.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.24.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.24.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.24.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.24.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.24.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.24.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.24.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.24.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.24.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.25.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.25.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.25.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.25.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.25.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.25.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.25.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.25.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.25.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.25.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.25.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.25.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.25.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.26.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.26.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.26.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.26.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.26.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.26.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.26.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.26.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.26.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.26.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.26.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.26.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.26.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.27.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.27.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.27.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.27.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.27.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.27.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.27.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.27.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.27.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.27.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.27.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.27.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.27.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.28.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.28.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.28.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.28.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.28.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.28.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.28.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.28.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.28.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.28.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.28.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.28.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.28.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.29.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.29.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.29.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.29.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.29.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.29.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.29.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.29.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.29.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.29.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.29.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.29.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.29.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.30.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.30.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.30.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.30.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.30.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.30.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.30.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.30.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.30.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.30.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.30.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.30.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.30.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.31.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.31.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.31.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.31.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.31.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.31.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.31.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.31.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.31.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.31.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.31.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.31.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.31.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.32.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.32.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.32.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.32.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.32.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.32.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.32.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.32.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.32.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.32.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.32.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.32.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.32.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.33.self_attn.q_proj.weight\n",
      "Frozen: language_model.model.layers.33.self_attn.k_proj.weight\n",
      "Unfrozen: language_model.model.layers.33.self_attn.v_proj.weight\n",
      "Frozen: language_model.model.layers.33.self_attn.o_proj.weight\n",
      "Frozen: language_model.model.layers.33.self_attn.q_norm.weight\n",
      "Frozen: language_model.model.layers.33.self_attn.k_norm.weight\n",
      "Frozen: language_model.model.layers.33.mlp.gate_proj.weight\n",
      "Frozen: language_model.model.layers.33.mlp.up_proj.weight\n",
      "Frozen: language_model.model.layers.33.mlp.down_proj.weight\n",
      "Frozen: language_model.model.layers.33.input_layernorm.weight\n",
      "Frozen: language_model.model.layers.33.post_attention_layernorm.weight\n",
      "Frozen: language_model.model.layers.33.pre_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.layers.33.post_feedforward_layernorm.weight\n",
      "Frozen: language_model.model.norm.weight\n"
     ]
    }
   ],
   "source": [
    "def selective_freeze(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        # 원하는 레이어만 True, 나머지는 False\n",
    "        if (\n",
    "            \"v_proj\" in name or \n",
    "            \"embed_tokens\" in name or\n",
    "            \"lm_head\" in name\n",
    "        ):\n",
    "            param.requires_grad = True\n",
    "            print(f\"Unfrozen: {name}\")\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "            print(f\"Frozen: {name}\")\n",
    "\n",
    "# 사용 예시\n",
    "selective_freeze(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839a1247",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Gemma3ForConditionalGeneration' object has no attribute 'print_trainable_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_trainable_parameters\u001b[49m()\n",
      "File \u001b[1;32mc:\\Users\\gkfua\\anaconda3\\envs\\space_0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1930\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Gemma3ForConditionalGeneration' object has no attribute 'print_trainable_parameters'"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ae60ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 796,244,352 | Total params: 4,300,079,472 | Trainable%: 18.52%\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable = 0\n",
    "    total = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        total += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable += num_params\n",
    "            # print(f\"[Trainable] {name}: {num_params}\")\n",
    "    print(f\"Trainable params: {trainable:,} | Total params: {total:,} | Trainable%: {100 * trainable / total:.2f}%\")\n",
    "\n",
    "# 예시\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6e8bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e10ffde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "space_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
